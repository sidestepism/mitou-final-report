\section{実装の概要}

本プロジェクトは，視覚（光学的）情報を適切な方法で音響化することにより，聴覚による空間知覚を可能にするデバイス「Sight」を開発することを目標とするものである．
九ヶ月間のプロジェクト実施期間を通じて我々は，「Sight」装着状態でより高度な空間知覚タスクの実行を可能にすべく，ヒトの視聴覚の特性に関する調査・検討を重ねつつ複数の視聴覚変換手法を開発した．
本セクションでは，各段階のバージョンにおいて提案された変換手法について，その技術的詳細と上記目標に対する実現度の評価を述べる．

いずれのバージョンにおいても，「Sight」は搭載した光学的センサからの入力（視覚情報）をパラメータ化し，このパラメータを用いてソフトウェアシンセサイザにより音響を合成するという処理のフローを採用する．
従って，視覚情報のパラメータ化，音響のパラメータ表現，および両者のパラメータの対応付けという三種類の変換の組み合わせにより入出力が対応させられることとなる．
以下，第一の変換にあたる「外界の認識」と第二・第三の変換にあたる「音響合成戦略」の二部に分けて各提案手法を詳説する．

また，本プロジェクトでは視聴覚変換手法のソフトウェア実装の他，装着用のハードウェアもデザイン・製作した．
本セクションでは「Sight」デバイスのハードウェアについても開発の経緯及び工夫点を解説する．


\subsection{外界の認識}

視覚的な外界の認識，すなわち光学的センサ入力の分析によるパラメータ化方式は，「Sight」システムが実現する「視覚」が提供する情報の内容および分量を規定する支配的要因である．
プロジェクト開始時において我々は，耳による直接的な目の代替を目指し，比較的低次の画像特徴量を多数抽出することで，通常目を通して受容するすべての視覚情報をパラメータ化することを試みた（バージョン1）．
次いで，物体把握に必要な視覚特徴量の適切な抽象化レベルを探求すべく，一般物体認識用深層ニューラルネットワークの中間層をパラメータ生成に利用する新手法を考案し，性能評価を行った（バージョン2）．

上記2バージョンを用いた実験の実施により，画像特徴量に基づくパラメータ化が学習の汎化に困難があると判明したため，周囲の空間構造の知覚を目標として再設定した．
まず，多くの動物がエコーロケーションと呼ばれる反響を利用したアクティブセンシングによって視覚的機能を実現している事例を参考として，深度センサを用いて周囲の物体との距離を単純に聴覚化することを試みた（バージョン3）．
次いで，生態光学の理論に基づき行動可能性の把握に基礎を置く視覚の機能的再定義を図り，支持平面や障害物といった空間の意味的構造の分析による，認知的に自然と考えられる高次のパラメータ化手法を開発した（バージョン4）．

\subsubsection{バージョン1: 低次の特徴点・局所特徴量(SURF)の分布}

hogehoge

\subsubsection{バージョン2: 一般物体認識のDNNを用いたモデル}

fugafuga

\subsubsection{バージョン3: 視野内数点の深さ情報}

piyopiyo

\subsubsection{バージョン4: 平面配置から行為可能性を解析}

foobar

\subsection{音響合成戦略}

\subsubsection{Granular Synthesisを用いた音響合成}

\subsubsection{Corpus-based Granular Synthesis}

\subsubsection{楽器，立体音響}

\subsection{ハードウェア}

