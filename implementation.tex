\section{実装の概要}

本プロジェクトは，視覚（光学的）情報を適切な方法で音響化することにより，聴覚による空間知覚を可能にするデバイス「Sight」を開発することを目標とするものである．
九ヶ月間のプロジェクト実施期間を通じて我々は，「Sight」装着状態でより高度な空間知覚タスクの実行を可能にすべく，ヒトの視聴覚の特性に関する調査・検討を重ねつつ複数の視聴覚変換手法を開発した．
本セクションでは，各段階のバージョンにおいて提案された変換手法について，その技術的詳細と上記目標に対する実現度の評価を述べる．

いずれのバージョンにおいても，「Sight」は搭載した光学的センサからの入力（視覚情報）をパラメータ化し，このパラメータを用いてソフトウェアシンセサイザにより音響を合成するという処理のフローを採用する．
従って，視覚情報のパラメータ化，音響のパラメータ表現，および両者のパラメータの対応付けという三種類の変換の組み合わせにより入出力が対応させられることとなる．
以下，第一の変換にあたる「外界の認識」と第二・第三の変換にあたる「音響合成戦略」の二部に分けて各提案手法を詳説する．

また，本プロジェクトでは視聴覚変換手法のソフトウェア実装の他，装着用のハードウェアもデザイン・製作した．
本セクションでは「Sight」デバイスのハードウェアについても開発の経緯及び工夫点を解説する．


\subsection{外界の認識}

視覚的な外界の認識，すなわち光学的センサ入力の分析によるパラメータ化方式は，「Sight」システムが実現する「視覚」体験において提供される情報の内容および分量を規定する支配的要因である．
プロジェクト開始時には我々は，耳による直接的な目の代替を目指し，比較的低次の画像特徴量を多数抽出することで，通常目を通して受容するすべての視覚情報をパラメータ化することを試みた（バージョン1）．
次いで，物体把握に必要な視覚特徴量の適切な抽象化レベルを探求すべく，一般物体認識用深層ニューラルネットワークの中間層をパラメータ生成に利用する新手法を考案し，性能評価を行った（バージョン2）．

上記2バージョンを用いた実験の実施により，画像特徴量に基づくパラメータ化が学習の汎化に困難があると判明したため，周囲の空間構造という抽象化された情報の知覚を目標として再設定した．
まず，イルカやコウモリをはじめ多くの動物がエコーロケーションと呼ばれる反響を利用したアクティブセンシングによって，障害物や他の動物の存在を知覚する視覚的機能を実現している事例を参考として，深度センサを用いて周囲の物体との距離を単純に聴覚化することを試みた（バージョン3）．
次いで，生態光学の理論に基づき行動可能性の把握に基礎を置く視覚の機能的再定義を図り，支持平面や障害物といった空間の基礎的な意味構造分析にもとづく，認知的に自然と考えられる高次のパラメータ化手法を開発した（バージョン4）．

\subsubsection{バージョン1: 低次の特徴点・局所特徴量(SURF)の分布}

\heading{アイディア}
ヒトを含む霊長類の視覚は，網膜で捉えた最低次の光学的刺激の配列を一次視覚野(V1)，二次視覚野(V2)等の脳領域で順次抽象化していく過程としてモデル化されている．
例えば一次視覚野のニューロンは網膜上の点との一対一対応（レチノトピー）を保っていることが知られており，入力に対してフィルタ処理に相当する応答が行われることで，空間的パターンや運動の知覚に必要な情報が抽出される．
また，近年の研究では，外部センサにより磁気刺激をマウスの一次視覚野に入力することで知覚代替(Sensory Substitution)が可能であること[Norimoto and Ikegaya, 2015]や，
視覚障害者がクリック音によるエコーロケーションを行う際に脳の視覚関連領域が活発に活動していること
[?]が知られており，
侵襲的処理や長期間の学習により，聴覚刺激など様々な入力を視覚野で処理できるようになる可塑性を脳が備えていると考えられている．

「Sight」最初期バージョンでは，こうした神経系でのボトムアップな情報処理及び可塑性に注目し，音響にエンコードされ耳へと入力される空間情報が視覚処理経路に直接バイパスされることによって，聴取による視覚が実現されることを目指した．
しかし，ヒトの聴覚の解像度（帯域幅）は，多数の視細胞からの入力を並列処理する視覚に比べて著しく小さく，光学刺激の空間パターンを単純に音響にエンコードしても，パターン認識などの抽象化に耐える情報量を聴取できるとは期待しづらい．
そこで，低次視覚野における情報の抽象化をソフトウェア的に模倣することで，神経系における情報処理との親和性を確保しつつ音響に変換される情報量の削減を図った．

\heading{実装}
入力デバイスとしては，目を模倣して一般的なRGB画像を出力するウェブカメラを用いる．
低次視覚野(V1)で行われるフィルタ処理とコンピュータビジョンにおける画像の局所特徴量計算との類似性に注目し，RGBカメラからの入力画像に対して特徴点検出器によって抽出された特徴点およびそれらの特徴ベクトルによって，入力の抽象化された表現とする．
今回の実装では，特徴点抽出アルゴリズムとしてはOpenCV 2.4実装のSURF（Speeded up robust features）を採用し，サイズ上位30個までの特徴点を音響生成に利用した．
各特徴点は64次元の特徴ベクトルで表現されるが，この次元をさらに削減するために，室内環境で収集した10,000個の特徴ベクトルに対して主成分分析を行い，上位成分のみを採用することで10次元のベクトル表現を得た．
10次元で表現された特徴ベクトルは，近傍の平均色および画像上の座標を加えて1つの音源に対応するオブジェクトとして保存され，音響生成ルーチンへと送信される．

\heading{評価}
本手法に基づく「Sight」を利用した評価・感想については音響変換の節で詳しく述べるため，本節では画像認識および特徴量に関する客観的な評価を概説する．

まず，一般的傾向としてSURF特徴点は画像上のエッジや角にあたる部分に検出されるため，滑らかな平面や壁面の画像を入力とした場合は少数で一様な特徴点が得られ，複雑な画像を入力とした場合は多数かつ雑多な成分分布を持つベクトル群が得られた．
このように視野の大域的特徴に関して特徴量分布が強い定性的相関を示すため，視野内の大規模な構造の変化（例えば壁面に顔を向けた場合，屋内外の変化など，平坦な地面と複雑な立ち木など）に対しては概ね鋭敏である．
一方，特徴点の検出位置は不安定性が強く，入力動画の連続したフレームでも僅かな頭部の動きや物体の移動にともなって，大きく上位特徴点の位置分布がずれる傾向が見られた．
これにより，特定の物体（机の上に置いたリンゴなど）に由来する特徴点であっても検出位置（リンゴのヘタ部分か側面か，など）によってベクトルが大幅に変動するので，物体の同一性を特徴ベクトルから推測するのは難しいことが分かった．

主成分分析による次元削減アルゴリズムに関しては，似た特徴点を大まかに似たベクトルとして変換する傾向が確認できた．
例えば，机の同一の縁上に検出された複数の特徴点は，いずれも似た局所的外見を目視できるが，これは次元削減後のベクトルの類似性にも反映されている．
一方，上述の通りある物体に関して様々な箇所の特徴点がフレームごとランダムに検出されるため，単一の物体上であっても雑多なベクトル群が出力される．
従って，直接ベクトルの分布を観察しても，物体を特徴点群のベクトルによって識別することは著しく難しいことが判明した．

本手法によって大まかに「何かがあるか」，「それは複雑な外見をしているか」，「それはどちらに動いているか」といった情報を捉えることは可能であるが，主に特徴点検出位置の不安定性および，特徴ベクトルの非特異性（様々な物体から似たベクトルが得られる）により，どのような状況であるかを知らせる情報は著しく欠落するという結論が得た．
検出する特徴点の個数を増やすことで検出位置の不安定性には対処が可能であるが，この場合聴取すべき情報量が増大するので認知的負荷は大きく増大する．
このように，低次の局所特徴量を用いるアプローチでは大幅な次元削減を行ったとしても，シーンを把握するために聴取すべき情報量は大きくなりすぎる傾向があるという知見から，以降のバージョンではより抽象度を高めたデータ分析によりシーン特異性を高めるアプローチに移行した。

ToDo: echo location引用
ToDo: Figureを入れる

\newpage

\subsubsection{バージョン2: 一般物体認識のDNNを用いたモデル}

fugafuga

\newpage

\subsubsection{バージョン3: 視野内数点の深さ情報}

\heading{アイディア}
バージョン2までの入力解析手法では，通常のカメラを入力として画像特徴量を様々な抽象度で抽出することで，「目で出来ることを全部耳で実現する」ことを究極的な目標として視覚的情報のパラメータ化を図った．
前述の評価の通り，それぞれ学習済みのパターンの再認はある程度可能だが，初めての環境で何らかの意味のある空間の分析や行動が可能になるような，汎化可能な学習は困難ということが判明した．
数週間以上の長期の学習フェイズを設けることで高度な学習が可能である可能性も残されていたが，プロジェクト期間中に有意義な結果を出すことは困難であると考え，10月上旬，耳による目の直接的代替を目標とすることをとりやめた．
我々は，そもそも実現すべき「見え」とは何であるかという概念分析を通じて，計測が困難である脳状態と見えの体験とを同一視するのではなく，見えているときに出来るべきことを実現できている状況こそが見えているということである，という機能的な再定義を行った．
プロジェクトではこれ以降，特に日常生活での行動の基盤となる，空間構造に関する確信を伴った知識の獲得を重視し，人間が実際に持っている感覚器官や情報処理機能とのアナロジーにこだわらずに当該機能の実現を追求することとした．
バージョン3では，周囲の障害物の知覚を空間把握の最小要件と考え，視野内の物体との距離情報をパラメータの生成に用いることとした．

\heading{実装}
従来のRGBカメラの代わりにデプスカメラ（ASUS Xtion PRO LIVE）をセンサーとして採用し，視野（水平58度）内の深度画像を取得する．
深度画像はそのままではピクセル数だけの情報量があり，音響合成に用いるにはRGB画像同様の問題があるので，前方および左右それぞれ２つずつの５水平角（プローブ）に関して，垂直方向の深度分布をベクトルとして扱うこととした．
これにより，５つのプローブに関する多次元ベクトルにより視野内の深度分布が代表される．
当然，プローブと重ならない障害物の距離情報は捉えることはできないが，頭部を細かく左右に振ることによって，容易にいずれかのプローブと方向を一致させることが出来る．

また，各プローブの特徴量は深度画像の垂直解像度（240ピクセル）に対応する高次元ベクトルであるが，バージョン1でのSURFのように人間に直接理解が不可能な抽象量ではなく，高さおよび深さという物理量と直接対応するため，あえて次元削減を行わなくても適切な音響マッピングによって容易に内容の把握は可能であると判断した．

\heading{評価}
情報量削減のためにプローブによる一部分の情報のみを抽出する処理は，バージョン1における特徴点抽出による画像代表と似ているが，いくつか重要な相違点がある．
第一に，プローブの位置は固定されているために，繰り返し頭を振ることによって何度でも同じ状態を復元することが出来る．この際，前後で画像に若干の変化が生じても，通常の室内環境で空間を構成する物体（壁面や机など）は数十cm程度の大きさを持つため，出力ベクトルにはほとんど影響しない．
第二に，特徴量が深さという物理量の分布で示されるため，その意味を学習し把握することがきわめて容易であり，また視野や物体の移動に伴うベクトル値の連続的な変化を捉えることも可能である．

一方，本手法では特に衝突しそうな距離の障害物などを発見するのは容易である代わりに，検出位置が離散化されているために空間中の連続的な構造の把握には向いていない．
また，距離のみを特徴量として抽出するため，壁面や大きい障害物のようにベクトル値に一様に現れる物体は顕著に表現できるが，ボールのような小さな障害物や，机のような重要な意味を持つ平面の存在は特徴量として露わに表現されない．
こうしたことから，本手法でのパラメータ化は壁面のような顕著な障害物の回避の支援に留まると考えられる．

\newpage

\subsubsection{バージョン4: 平面配置から行為可能性を解析}

foobar

\subsection{音響合成戦略}

\subsubsection{バージョン1: Granular Synthesisを用いた音響合成}

\subsubsection{バージョン2: サンプリング音の変調による音響合成}

\subsubsection{バージョン3: FFTによる周波数空間フィルタリング}

\subsubsection{バージョン4-1: Corpus-based Granular Synthesis}

\subsubsection{バージョン4-2: 楽器音による音楽的音響合成}

\subsubsection{立体音響の実装に関して}

\subsection{ハードウェア}

