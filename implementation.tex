\section{実装の概要}

本プロジェクトは，視覚（光学的）情報を適切な方法で音響化することにより，聴覚による空間知覚を可能にするデバイス「Sight」を開発することを目標とするものである．
九ヶ月間のプロジェクト実施期間を通じて我々は，「Sight」装着状態でより高度な空間知覚タスクの実行を可能にすべく，ヒトの視聴覚の特性に関する調査・検討を重ねつつ複数の視聴覚変換手法を開発した．
本セクションでは，各段階のバージョンにおいて提案された変換手法について，その技術的詳細と上記目標に対する実現度の評価を述べる．

いずれのバージョンにおいても，「Sight」は搭載した光学的センサからの入力（視覚情報）をパラメータ化し，このパラメータを用いてソフトウェアシンセサイザにより音響を合成するという処理のフローを採用する．
従って，視覚情報のパラメータ化，音響のパラメータ表現，および両者のパラメータの対応付けという三種類の変換の組み合わせにより入出力が対応させられることとなる．
以下，第一の変換にあたる「外界の認識」と第二・第三の変換にあたる「音響合成戦略」の二部に分けて各提案手法を詳説する．

また，本プロジェクトでは視聴覚変換手法のソフトウェア実装の他，装着用のハードウェアもデザイン・製作した．
本セクションでは「Sight」デバイスのハードウェアについても開発の経緯及び工夫点を解説する．


\subsection{外界の認識}

視覚的な外界の認識，すなわち光学的センサ入力の分析によるパラメータ化方式は，「Sight」システムが実現する「視覚」体験において提供される情報の内容および分量を規定する支配的要因である．
プロジェクト開始時には我々は，耳による直接的な目の代替を目指し，比較的低次の画像特徴量を多数抽出することで，通常目を通して受容するすべての視覚情報をパラメータ化することを試みた（バージョン1）．
次いで，物体把握に必要な視覚特徴量の適切な抽象化レベルを探求すべく，一般物体認識用深層ニューラルネットワークの中間層をパラメータ生成に利用する新手法を考案し，性能評価を行った（バージョン2）．

上記2バージョンを用いた実験の実施により，画像特徴量に基づくパラメータ化が学習の汎化に困難があると判明したため，周囲の空間構造という抽象化された情報の知覚を目標として再設定した．
まず，イルカやコウモリをはじめ多くの動物がエコーロケーションと呼ばれる反響を利用したアクティブセンシングによって，障害物や他の動物の存在を知覚する視覚的機能を実現している事例を参考として，深度センサを用いて周囲の物体との距離を単純に聴覚化することを試みた（バージョン3）．
次いで，生態光学の理論に基づき行動可能性の把握に基礎を置く視覚の機能的再定義を図り，支持平面や障害物といった空間の基礎的な意味構造分析にもとづく，認知的に自然と考えられる高次のパラメータ化手法を開発した（バージョン4）．

\subsubsection{バージョン1: 低次の特徴点・局所特徴量(SURF)の分布}

\heading{アイディア}
ヒトを含む霊長類の視覚は，網膜で捉えた最低次の光学的刺激の配列を一次視覚野(V1)，二次視覚野(V2)等の脳領域で順次抽象化していく過程としてモデル化されている．
例えば一次視覚野のニューロンは網膜上の点との一対一対応（レチノトピー）を保っていることが知られており，入力に対してフィルタ処理に相当する応答が行われることで，空間的パターンや運動の知覚に必要な情報が抽出される．
また，近年の研究では，外部センサにより磁気刺激をマウスの一次視覚野に入力することで知覚代替(Sensory Substitution)が可能であること[Norimoto and Ikegaya, 2015]や，
視覚障害者がクリック音によるエコーロケーションを行う際に脳の視覚関連領域が活発に活動していること
[?]が知られており，
侵襲的処理や長期間の学習により，聴覚刺激など様々な入力を視覚野で処理できるようになる可塑性を脳が備えていると考えられている．

「Sight」最初期バージョンでは，こうした神経系でのボトムアップな情報処理及び可塑性に注目し，音響にエンコードされ耳へと入力される空間情報が視覚処理経路に直接バイパスされることによって，聴取による視覚が実現されることを目指した．
しかし，ヒトの聴覚の解像度（帯域幅）は，多数の視細胞からの入力を並列処理する視覚に比べて著しく小さく，光学刺激の空間パターンを単純に音響にエンコードしても，パターン認識などの抽象化に耐える情報量を聴取できるとは期待しづらい．
そこで，低次視覚野における情報の抽象化をソフトウェア的に模倣することで，神経系における情報処理との親和性を確保しつつ音響に変換される情報量の削減を図った．

\heading{実装}
入力デバイスとしては，目を模倣して一般的なRGB画像を出力するウェブカメラを用いる．
低次視覚野(V1)で行われるフィルタ処理とコンピュータビジョンにおける画像の局所特徴量計算との類似性に注目し，RGBカメラからの入力画像に対して特徴点検出器によって抽出された特徴点およびそれらの特徴ベクトルによって，入力の抽象化された表現とする．
今回の実装では，特徴点抽出アルゴリズムとしてはOpenCV 2.4実装のSURF（Speeded up robust features）を採用し，サイズ上位30個までの特徴点を音響生成に利用した．
各特徴点は64次元の特徴ベクトルで表現されるが，この次元をさらに削減するために，室内環境で収集した10,000個の特徴ベクトルに対して主成分分析を行い，上位成分のみを採用することで10次元のベクトル表現を得た．
10次元で表現された特徴ベクトルは，近傍の平均色および画像上の座標を加えて1つの音源に対応するオブジェクトとして保存され，音響生成ルーチンへと送信される．

\heading{評価}
本手法に基づく「Sight」を利用した評価・感想については音響変換の節で詳しく述べるため，本節では画像認識および特徴量に関する客観的な評価を概説する．

まず，一般的傾向としてSURF特徴点は画像上のエッジや角にあたる部分に検出されるため，滑らかな平面や壁面の画像を入力とした場合は少数で一様な特徴点が得られ，複雑な画像を入力とした場合は多数かつ雑多な成分分布を持つベクトル群が得られた．
このように視野の大域的特徴に関して特徴量分布が強い定性的相関を示すため，視野内の大規模な構造の変化（例えば壁面に顔を向けた場合，屋内外の変化など，平坦な地面と複雑な立ち木など）に対しては概ね鋭敏である．
一方，特徴点の検出位置は不安定性が強く，入力動画の連続したフレームでも僅かな頭部の動きや物体の移動にともなって，大きく上位特徴点の位置分布がずれる傾向が見られた．
これにより，特定の物体（机の上に置いたリンゴなど）に由来する特徴点であっても検出位置（リンゴのヘタ部分か側面か，など）によってベクトルが大幅に変動するので，物体の同一性を特徴ベクトルから推測するのは難しいことが分かった．

主成分分析による次元削減アルゴリズムに関しては，似た特徴点を大まかに似たベクトルとして変換する傾向が確認できた．
例えば，机の同一の縁上に検出された複数の特徴点は，いずれも似た局所的外見を目視できるが，これは次元削減後のベクトルの類似性にも反映されている．
一方，上述の通りある物体に関して様々な箇所の特徴点がフレームごとランダムに検出されるため，単一の物体上であっても雑多なベクトル群が出力される．
従って，直接ベクトルの分布を観察しても，物体を特徴点群のベクトルによって識別することは著しく難しいことが判明した．

本手法によって大まかに「何かがあるか」，「それは複雑な外見をしているか」，「それはどちらに動いているか」といった情報を捉えることは可能であるが，主に特徴点検出位置の不安定性および，特徴ベクトルの非特異性（様々な物体から似たベクトルが得られる）により，どのような状況であるかを知らせる情報は著しく欠落するという結論が得た．
検出する特徴点の個数を増やすことで検出位置の不安定性には対処が可能であるが，この場合聴取すべき情報量が増大するので認知的負荷は大きく増大する．
このように，低次の局所特徴量を用いるアプローチでは大幅な次元削減を行ったとしても，シーンを把握するために聴取すべき情報量は大きくなりすぎる傾向があるという知見から，以降のバージョンではより抽象度を高めたデータ分析によりシーン特異性を高めるアプローチに移行した。

ToDo: echo location引用
ToDo: Figureを入れる


\subsubsection{バージョン2: 一般物体認識のDNNを用いたモデル}

\heading{アイディア}

一般物体認識の領域で非常に良好な成績を納めているDeep Neural Networkというモデルがある．
これは既存のフィードフォワード型のニューラルネットワークを多層化したものであるが，
シンプルに多層化するとバックプロパゲーション学習の効果が薄れてしまうという欠点を克服するため，
初期値の決定に関してオートエンコーダなど新しい特徴抽出の仕組みを取り入れたものである．
このような既存のモデルは，ILSVRCという一般物体認識の性能コンテストに
応募されたたくさんのモデルおよびそれを解説した論文に詳しく記されている．

このようなDNNの入力層に画像を与えてフィードフォワード計算を行うと，
ニューラルネットワークの最終層の出力には
分類の結果に関する情報が生成される．
ILSVRCでは1000カテゴリの物体認識を行うので，この出力は1000次元である．
最初は，この分類カテゴリ情報を元に音に変換する実装を行ったが，
この方式には2つの欠点があった．
1つは「学習したカテゴリ以外の者に拡張することができない」というものである．
見たことのないものは，角度や見え方に寄って，似た複数のもののカテゴリ間で振動したり
全く見当違いのカテゴリとして認識されたりしてしまう．
もう1つの問題は「カテゴリ内の特徴の違いを認識することができない」というものである．
カテゴリに含まれるものであっても，
例えば目の前にあるりんごがどんなりんごなのか，
目の前にいる女性がどんな女性なのかという情報は
最終層の情報では捨象されている．
これらの欠点を解決するため，最終層ではなくその一つ手前の中間層の情報を用いることを着想した．
この層の情報は，1層のニューラルネットによって統合されカテゴリ情報になる．
このパターン認識の処理を人間の脳に行わせることで，
これらの問題を解決するだけではなく，
統合した認識結果を聞くのではなく，視覚情報の統合・認識を人間に行わせることで
より「見ている」という体験に近い体験を引き起こすことができるのではないかと考えた．

\heading{実装}

PythonのDeep Neural Network実装で多く使われているCaffeというライブラリを用いて実装をこなった．モデルとしては，2012年，HintonらのILSVRCに応募され公開されているpre-trainedモデルを用いた．このモデルは最終層が1000次元，その1つ前のフルコネクションレイヤーが4096次元であった．
この中間層の情報を音生成の10のパラメータへの変換は自明ではない．今回は，値の大きい代表的な次元を抽出し，その次元の番号 (0-4095) を，3進数で表し，その各桁を10のパラメータそれぞれに割り当てるという方針をとった．初期のバージョンでは目の前に1つのものを判別することしかできないという欠点があったため，認識の対象となる物体のありそうな領域を検出するアルゴリズムを用意し，その検出結果についてそれぞれDNNによる認識を行うという2段構えの実装を行った．

\heading{評価}

東京大学制作展EXTRAで，このバージョンの展示を行った．
多くの体験者は1-2分のレクチャーのあと，人に対応した音を覚えて，目を閉じた状態であっても目の前にいる人を当てることができた．

もちろん目の前にあるものを認識することは，人の顔を認識したり，手の届く範囲で完結する作業を行う上では重要であるが，空間の広がりや，周りに人がいるかなどに関しては全く情報を得ることができず，日常生活に典型的な歩行などの行動に関してはほとんど周囲の人の補助に頼るしか方法がない．そこで次のバージョンでは，空間の広がりに関する情報を音に変換するモジュールを実装し，これらを組み合わせることで日常のより広範な動作をカバーすることを考えた．

\subsubsection{バージョン3: 視野内数点の深さ情報}

\heading{アイディア}
バージョン2までの入力解析手法では，通常のカメラを入力として画像特徴量を様々な抽象度で抽出することで，「目で出来ることを全部耳で実現する」ことを究極的な目標として視覚的情報のパラメータ化を図った．
前述の評価の通り，それぞれ学習済みのパターンの再認はある程度可能だが，初めての環境で何らかの意味のある空間の分析や行動が可能になるような，汎化可能な学習は困難ということが判明した．
数週間以上の長期の学習フェイズを設けることで高度な学習が可能である可能性も残されていたが，プロジェクト期間中に有意義な結果を出すことは困難であると考え，10月上旬，耳による目の直接的代替を目標とすることをとりやめた．
我々は，そもそも実現すべき「見え」とは何であるかという概念分析を通じて，計測が困難である脳状態と見えの体験とを同一視するのではなく，見えているときに出来るべきことを実現できている状況こそが見えているということである，という機能的な再定義を行った．
プロジェクトではこれ以降，特に日常生活での行動の基盤となる，空間構造に関する確信を伴った知識の獲得を重視し，人間が実際に持っている感覚器官や情報処理機能とのアナロジーにこだわらずに当該機能の実現を追求することとした．
バージョン3では，周囲の障害物の知覚を空間把握の最小要件と考え，視野内の物体との距離情報をパラメータの生成に用いることとした．

\heading{実装}
従来のRGBカメラの代わりにデプスカメラ（ASUS Xtion PRO LIVE）をセンサーとして採用し，視野（水平58度）内の深度画像を取得する．
深度画像はそのままではピクセル数だけの情報量があり，音響合成に用いるにはRGB画像同様の問題があるので，前方および左右それぞれ２つずつの５水平角（プローブ）に関して，垂直方向の深度分布をベクトルとして扱うこととした．
これにより，５つのプローブに関する多次元ベクトルにより視野内の深度分布が代表される．
当然，プローブと重ならない障害物の距離情報は捉えることはできないが，頭部を細かく左右に振ることによって，容易にいずれかのプローブと方向を一致させることが出来る．

また，各プローブの特徴量は深度画像の垂直解像度（240ピクセル）に対応する高次元ベクトルであるが，バージョン1でのSURFのように人間に直接理解が不可能な抽象量ではなく，高さおよび深さという物理量と直接対応するため，あえて次元削減を行わなくても適切な音響マッピングによって容易に内容の把握は可能であると判断した．

\heading{評価}
情報量削減のためにプローブによる一部分の情報のみを抽出する処理は，バージョン1における特徴点抽出による画像代表と似ているが，いくつか重要な相違点がある．
第一に，プローブの位置は固定されているために，繰り返し頭を振ることによって何度でも同じ状態を復元することが出来る．この際，前後で画像に若干の変化が生じても，通常の室内環境で空間を構成する物体（壁面や机など）は数十cm程度の大きさを持つため，出力ベクトルにはほとんど影響しない．
第二に，特徴量が深さという物理量の分布で示されるため，その意味を学習し把握することがきわめて容易であり，また視野や物体の移動に伴うベクトル値の連続的な変化を捉えることも可能である．

一方，本手法では特に衝突しそうな距離の障害物などを発見するのは容易である代わりに，検出位置が離散化されているために空間中の連続的な構造の把握には向いていない．
また，距離のみを特徴量として抽出するため，壁面や大きい障害物のようにベクトル値に一様に現れる物体は顕著に表現できるが，ボールのような小さな障害物や，机のような重要な意味を持つ平面の存在は特徴量として露わに表現されない．
こうしたことから，本手法でのパラメータ化は壁面のような顕著な障害物の回避の支援に留まると考えられる．


\subsubsection{バージョン4: 平面配置から行為可能性を解析}

\heading{アイディア}
バージョン3では把握が容易な空間特徴量のパラメータ化に成功したが，一方で実現された行動は大型障害物の回避に限られ，通常人間が把握できる空間構造に比べて貧弱な情報量しか表現できないものであった．
しかし，各点での法線方向などの情報量を単純に追加することは，ベクトルの複雑化を招き情報把握を困難にする懸念が大きい．
ここで再び，我々が空間を見て知覚している時には具体的に何を見ているのか，主にJames J. Gibsonらの生態光学のアプローチを参考に検討することで，日常的な知覚様式を反映した情報抽出および情報量削減の手法を考案した．

生態光学において，人間はまず立体視にもとづいて空間の３次元配置を把握して，そこから自身の行動を演繹的に決定するのではなく，視点移動にともなう遮蔽縁の変化や肌理の勾配といった光学的要素から，支持平面や障害物の境界など空間に埋め込まれた行動の可能性（アフォーダンス）を直接知覚するとされている．
直接知覚の神経学的基礎付けについては議論があるものの，アフォーダンス理論には強い実証的支持や理論的重要性が与えられている．
我々は，この理論を大きく単純化し，人間は視覚を用いて４種類（移動の障害・平面上の支持・衝突・把持）の基本的な行動可能性を直接知覚できるものと仮定した．
この４語彙はそれぞれ，「壁面」「支持平面」「大型ボリューム」「小型ボリューム」という３次元構造に対応する．
次いで，直接知覚の具体的仕組みにあたる部分を計算機によりソフトウェア的に実現することで，「ここで移動が障害される」「ここに物を置いたり腰を下ろせる」といった意味を直接知覚＝パラメータ化することを目論んだ．

\heading{実装}
バージョン3同様，センサとしてはデプスカメラを用いて，深度画像をポイントクラウド（デプスセンサの測定値を3次元空間上の点群としたデータ構造）に変換することで処理への入力とした．
点群からアフォーダンス語彙に対応する構造を抽出するために，平面検出（plane detection）の処理を行って壁面と支持平面を抽出し，残った構造にクラスタリングを行うことで，大型および小型のボリューム群を抽出する．
壁面と支持平面とは，その法線方向が45度よりも水平寄り（壁面）か垂直寄り（支持平面）かによって分別し，それぞれ平面の中心位置，法線方向，第一主軸と第二主軸，主軸沿いの二次元的広がりとをパラメータとした．
主軸ベクトルは平面に属する点群の座標値に対して主成分分析を行うことで計算し，主軸沿いの広がりは各主成分に関する標準偏差で代表する．
各ボリュームについても，クラスタごとに同様のパラメータを計算し，その主軸沿いの体積によって大きさを分類する．
ポイントクラウドの処理はPoint Cloud Library（PCL）を採用し，平面検出および点群クラスタリングは組み込みの機能を用いて実装した．
また，パラメータの類似性に基づいて同一エンティティをフレーム間トラッキングを行っている．

この他，大まかな空間の広がりを代表する値として，視野の左右半分ずつに関して深度値の平均値を計算して，アフォーダンス語彙とは独立した空間情報として音響合成ルーチンに送信した．

\heading{評価}
本手法はバージョン3とは異なり，左右の深度平均値を除いて生のセンサデータを音響生成に利用することは一切行わず，デプスデータの統合により，意味のある空間構造ごとに音源となるオブジェクトを生成している．
従って，上述の通り行動に強く関連するアフォーダンスの情報が直接得られる他，平面検出は結果に頑強性があり，視線方向や位置の変化に対して分析結果が安定で，またパラメータに連続的に反映される点で特徴的である．
例えば，壁を前にして左右に首をゆっくりと振ると，壁に対応するオブジェクトの法線方向パラメータが連続的に変化する様子を捉えることが出来る．
一方，ボリュームの検出については，毎フレーム平面に収まらなかった点群に対するクラスタリングを行うため，特に複数のボリュームが並んでいる場合に分節結果に若干の不安定性がある．
特に中程度のボリュームが複数近接している場合，複数の中程度ボリュームとして解釈されるフレームと一つの大規模ボリュームとして解釈されるフレームが交互に現れるケースがあり，何らかの対策が望まれる．

四種類にまで空間構造を単純化しているのに加えて，パラメータはいずれも単純に把握できる物理量として与えているので，通常視野内に映っているオブジェクト（典型的なシーンでは5,6個程度）は全てのパラメータが適切な聴覚化により把握可能であると期待される．
これによりバージョン３に比べて多くのタスクが実行可能となるが，一方で生のセンサデータは音響生成ルーチンから排除されるため，例えばオブジェクトの形状や種類について音響から詳しい情報を得ることは原理的に不可能となっている．
これは，プロジェクト当初の「耳による視覚の代替」という目標には反する結果であるが，適用可能な状況や情報量に制約を設けることで，代わりにより正確かつ安定した知覚体験を与え，新たな知覚様式についての報告を得ることを意図したものである．

\subsection{音響合成戦略}

\subsubsection{バージョン1: Granular Synthesisを用いた音響合成}

\subsubsection{バージョン2: サンプリング音の変調による音響合成}

\subsubsection{バージョン3: FFTによる周波数空間フィルタリング}

\subsubsection{バージョン4-1: Corpus-based Granular Synthesis}

\subsubsection{バージョン4-2: 楽器音による音楽的音響合成}

\subsection{ハードウェア}

